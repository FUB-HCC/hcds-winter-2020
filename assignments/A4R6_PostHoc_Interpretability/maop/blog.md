# Title of your post
> **Name:** `maop` Marc O.
> **Session:** [06 Exercise - Post-hoc Interpretability](https://github.com/FUB-HCC/hcds-winter-2020/wiki/06_exercise)   
----

## R6 - Reflection
> **Date:** 13.12.2020 - 00:15 AM *(Due: 15.12.2020 - 03:00 PM)*<br>
> **Podcast:** "Judea Pearl: Causal Reasoning, Counterfactuals, Bayesian Networks, and the Path to AGI" (Lex Fridman)

### üó®Ô∏è&nbsp; "How does the podcast inform your understanding of human-centered data science?"  
_Answer in at least 2-3 complete sentences_

I have learned that there is no clear definition of intelligence. That means that faked intelligence is considered as intelligence. I also learned about the challenges conditional probability is facing: Conditional probability is requiring one aspect to be static. This can not always be guaranteed in uncontrolled experiments (e.g driver deciding to turn on/switch off the autopilot).


### ‚ùì&nbsp; Questions
1. I have a difficult time understanding how one can quantify parameters (that are neccessary to model a situation) that have been unquantified before? Who decides that this might be a good quantification of different parameters and how can you be sure that they are not correlating with each other. In order create a model you have to generalize aspects. Is there a way to make sure you are not neglecting aspects that might've been suuuper important later but did not show any relevance when modelling?
1. Also I feel like it might be difficult to investigate causes of a specific phenomena when you use the "representation first, discovery second" approach?

***

## A4 - Transparency
Please, put everything regarding `A4` into the `blog.md` file of last week!
