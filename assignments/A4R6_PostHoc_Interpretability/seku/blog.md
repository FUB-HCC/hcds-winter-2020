# A4R6
> **Name:** `seku` Sebastian K.
> **Session:** [06 Exercise - Post-hoc Interpretability](https://github.com/FUB-HCC/hcds-winter-2020/wiki/06_exercise)   
----

## R6 - Reflection
> **Date:** 15.12.2020 - 11:00 AM *(Due: 15.12.2020 - 03:00 PM)*<br>
> **Podcast:** "Judea Pearl: Causal Reasoning, Counterfactuals, Bayesian Networks, and the Path to AGI" (Lex Fridman)

### üó®Ô∏è&nbsp; "How does the podcast inform your understanding of human-centered data science?"  
_Answer in at least 2-3 complete sentences_

While human intelligence is based on the ability to reason with cause and effect, machine intelligence is based on statistical models derived from 
correlation in data in the form of static conditional probabilites. That's because we are trying to fit problems into mathematical equasions and solve 
them in that way, which on one hand helps to understand research and makes it transparent and interpretable, but on the other hand abstracts it in a 
way that is rather far from the complex reality. This is espacially the case because we're not even able to define a clear definition of intelligence 
itself.

### ‚ùì&nbsp; Questions
1. How do we manage to find a mathematical formulation of every complex problem that has to be researched/understood?

**Why:** Judea Pearl mentioned, that to do good research it is important to formulate a problem in a mathematical manner, but I don't how this could be achieved 
for every problem or wether this is even possible.

***

## A4 - Transparency
Please, put everything regarding `A4` into the `blog.md` file of last week!
