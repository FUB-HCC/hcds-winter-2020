# Title of your post
> **Name:** `xiyu` Xin Y.
> **Session:** [06 Exercise - Post-hoc Interpretability](https://github.com/FUB-HCC/hcds-winter-2020/wiki/06_exercise)   
----

## R6 - Reflection
> **Date:** 14.12.2020 - 14:04 PM *(Due: 15.12.2020 - 03:00 PM)*<br>
> **Podcast:** "Judea Pearl: Causal Reasoning, Counterfactuals, Bayesian Networks, and the Path to AGI" (Lex Fridman)

### üó®Ô∏è&nbsp; "How does the podcast inform your understanding of human-centered data science?"  
_Answer in at least 2-3 complete sentences_

Current mainstream machine learning is built on statistical learning and computational theory, but is generally considered to be very different from general-purpose intelligence (AGI). For example, humans are capable of reasoning, but current mainstream machine learning is not. The working mechanism behind machine learning algorithms are almost based on the correlation and data fitting. The lack of robustness and interpretability may lay in its association-driven character and algorithms do not distinguish between causal and spurious associations (such as confounding and selection bias) in the data presentation. Therefore, Judea Pearl proposed causal inference as a powerful modeling tool for explanatory analysis that can help recover causal associations in data and be used to guide machine learning towards explainable and stable predictions.

### ‚ùì&nbsp; Questions
1. I have some difficulties to understand the concept "metaphor". It sounds for me more like a philosophical or literary concept. Prof. Pearl described metaphor as a way of learning knowledge of new domain by referring them to similar domains which we are familiar with. Then what is the difference between metaphor and transfer learning? How to do the mapping from one domain to another domain? How to evaluate if the metaphor is acceptable or "good" enough when we don't have enough data in a new domain or don't have suitable labels for data? 
2. This is my personal confusion and I am currently even unable to formulate a suitable well-defined question. Is it essential to find out every causal relation behind a machine learning task? (Or how do we know for what task there IS actually a causal effect behind). Because we know that people are not rational at all, even though we somehow have the "rationale assumption" (at least in economics). Somehow we can not always find a causal relation behind intention and outcomes. Or some causes are more personal and emotional which is quite difficult to even observed, let alone measured. Additionally, people learn by imitating as a baby, which seems just like the curve fitting in machine learning. In human science, some causes may be just "added" artificially afterwards. 

***

## A4 - Transparency
Please, put everything regarding `A4` into the `blog.md` file of last week!