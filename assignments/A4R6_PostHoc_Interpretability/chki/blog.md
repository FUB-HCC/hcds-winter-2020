# Judea Pearl
> **Name:** `chki` Alexa S.
> **Session:** [06 Exercise - Post-hoc Interpretability](https://github.com/FUB-HCC/hcds-winter-2020/wiki/06_exercise)   
----

## R6 - Reflection
> **Date:** 11.12.2020 - 04:00 PM *(Due: 15.12.2020 - 03:00 PM)*<br>
> **Podcast:** "Judea Pearl: Causal Reasoning, Counterfactuals, Bayesian Networks, and the Path to AGI" (Lex Fridman)

### üó®Ô∏è&nbsp; "How does the podcast inform your understanding of human-centered data science?"  

_Answer in at least 2-3 complete sentences_

I found the podcast difficult to follow in parts. I felt, that too many topics were covered in too little time (with quick jumps between them) so that I couldn't wrap my head around each topic. Maybe I listened to the podcast too late at night.

I learned that presented data/facts may not be enough to understand the underlaying mechanisms ("looking at the facts doesn't give you the strings behind the facts" - around minute 55). In addition to that, I learned that a model can be useful to think about and examine questions that we cannot examine in reality (e.g changing the blood pressure in a specific way and observing the effect on mortality).
Also, I found the rather philosphical discussion about consciousness of machines and religion for machines very interesting.

### ‚ùì&nbsp; Questions

1. Can we use Metaphors to increase the interpretability of our models? According to Judea, Metaphors are a strong tool that help us to understand new problems by mapping them to known problems. I could imagine, that they can be applied to achieve interpretability.
1. (How exactly does the do-operator work? They seem to be an important concept but it was hard for me to understand these operators by listening to the podcast.)

## A4 - Transparency
Please, put everything regarding `A4` into the `blog.md` file of last week!
