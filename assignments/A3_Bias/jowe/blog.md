# Assignment 3
> **Date:** 23.11.2020 - 21:28 PM *(Due: 01.12.2020 - 03:00 PM)*
> **Name:** `jowe` Jonas W.
> **Session:** [03 Exercise - Bias](https://github.com/FUB-HCC/hcds-winter-2020/wiki/03_exercise)   
----

## R3 - Reflection
> Article: Algorithmic Profiling of Job Seekers in Austria: How Austerity Politics Are Made Effective

### üó®Ô∏è&nbsp; "How does the video inform your understanding of human centered data science?"  
_In at least 2-3 full sentences, answer the question "How does the video inform your understanding of human centered data science?"._

When creating any type of computer system it is neccessary to view the system as the complete socio-technical system it is, instead of only a technical one, which may be completely correct or calculcated. It is important to think about and discuss the possible implications any system might create when introduced into the context in which it is supposed to be used, as well as any current factors coming from this context which might have influenced the creation of said system. 

### ‚ùì&nbsp; Questions
_Using full sentences, list at least one question that this video raised in your mind, and say why it caused you to ask this question_

1. While the authors raise very valid points about specific problems of the AMS Algorithm I am unsure whether I agree with the points they raise about whether a "workfare" state is morally good or bad or whether putting job seekers in a category of low chance of employment may be a self fulfilling prophecy. While these points are surely worth discussing, the question it raises for me is, if every person in every position should be responsible or moraly obligated to break biased structures in their society. In my opinion, whether the "workfare" state is the "right" state structure should not be a question influencing the creators of descriptive algorithms like the one at the AMS in their work. The goal of the algorithm was to predict the chance of a specific applicant to get hired in a given timeframe in the current society. If the system, which the algorithm describes is biased against groups of people, shouldn't the algorithm display that? It is objectively wrong to not hire applicants based on factors like age, gender, etc. but in my opinion the authors mix up their valid criticism of the inner workings of the algorithms with a system critique which may not have a place in this discussion but should and must be continued in another seperate place. Whether this is the right approach and leads to actionable results rather than stopping the use and improvements of such algorithms due to systematic problems which aren't influenceable in the same timeframe and by the same people, is a question worth thinking about and discussing, in my opinion.

***

## A3 - Wikipedia, ORES, and BIAS

**Repository:** `<add link to our repo here>`

### Reflections and implications

Write about `350` words, reflecting on what you have learned, what you found, what (if anything) surprised üò≤ you about your findings, and/or what theories you have about why any biases might exist (if you find they exist). Please also include any questions this assignment raised for you about bias, Wikipedia, or machine learning.

_Your 350 words_

1. _Your question 1?_
1. _Your question 2?_

### Questions

Pleas answer the following questions with at least 2-3 sentences each.

1. What biases did you expect to find in the data (before you started working with it), and why?
    * _answer_
1. What (potential) sources of bias did you discover or introduce during data processing and analysis?
    * _answer_
1. What might your results suggest about (English) Wikipedia as a data source?
    * _answer_
1. What might your results suggest about the internet and global society in general?
    * _answer_
1. Can you think of a realistic data science research situation where using these data (to train a model, perform a hypothesis-driven research, or make business decisions) might create biased or misleading results, due to the inherent gaps and limitations of the data?
    * _answer_
1. Can you think of a realistic data science research situation where using these data (to train a model, perform a hypothesis-driven research, or make business decisions) might still be appropriate and useful, despite its inherent limitations and biases?
    * _answer_
1. How might a researcher supplement or transform this dataset to potentially correct for the limitations/biases you observed?
    * _answer_
