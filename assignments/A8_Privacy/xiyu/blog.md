# 11 Session - Privacy and Security
> **Name:** `xiyu` Xin Y.
> **Session:** [11 Session - Privacy and Security](https://github.com/FUB-HCC/hcds-winter-2020/wiki/10_exercise)   
----

## Preparation

_Franziska Boenisch completed her Bachelor and Master's degrees in Computer Science from Free University of Berlin and has plenty of academic research experience in machine learning as well as data privacy. She is now a research associate of Fraunhofer with focus on privacy preserving Machine Learning and data protection._ 

1. In the paper (page 2) is mentioned that "ensembling different learning systems is one of the ways to improve ML security (e.g., having an ensemble of learners vote for a prediction can make this prediction private". I don't really understand the sentence by:<br>
    1.1 Why using emsemble learning systems will improve security? By making the system more intransparent and complicated?
    1.2 How can we combine different learning systems? Will the ensembling decrease the system's performance?
1. In the section 4, it seems that there are generally two approaches for model assurance: data protection (pre-model) and system auditing (post-model). I was wondering if there is many systems which continously learn and train, which means there is no clearly separating phases of training and testing. Therefore, the methods mentioned in Section 4.1 and 4.2 seem cannot protect system from attacks at runtime well. Is there any protecting system which recognizes unwilling attacks or manipulation at runtime?


## Summary
_approximately 250 words_




## Mind Map

* My Mind Map 


## Question


## Takeways

