# 11 Exercise - Privacy
> **Name:** `seku` Sebastian K.
> **Session:** [11 Exercise - Privacy](https://github.com/FUB-HCC/hcds-winter-2020/wiki/11_exercise)   
----

## Preparation

Franziska Boenisch is a research associate at Frauenhofer AISEC, working on topics related to Privacy Preserving Machine Learning, Data Protection, and Intellectual Property Protection for Neural Networks, and PhD Student at FU Berlin. Her presentation will cover common privacy threat spaces in data analysis methods with a special focus on machine learning.

1. While researchers are trying to develop policies and guidlines for safer and better ML Systems, tech firms and/or corporations are developing systems that exploit a lack of legislation to the disadvantage of data creators. Due to the tremendous impact of those systems in terms of privacy violation, shouldn't there be stricter laws for ML System development or do you have a better solution?
1. Do you have an idea or some kind of concept for a security policy that would be sufficiently abstact to be understandable for humans as well as  sufficiently low  abstract to be understandable for machines?


## Summary
_approximately 250 words_


## Mind Map

* Add your png file here please.
* Please name your png file as `<alsc>_mind-map.png`.

## Question
...?

## Takeways
...
