# 11 Exercise - Privacy
> **Name:** `seku` Sebastian K.
> **Session:** [11 Exercise - Privacy](https://github.com/FUB-HCC/hcds-winter-2020/wiki/11_exercise)   
----

## Preparation

Franziska Boenisch is a research associate at Frauenhofer AISEC, working on topics related to Privacy Preserving Machine Learning, Data Protection, and Intellectual Property Protection for Neural Networks, and PhD Student at FU Berlin. Her presentation will cover common privacy threat spaces in data analysis methods with a special focus on machine learning.

1. While researchers are trying to develop policies and guidlines for safer and better ML Systems, tech firms and/or corporations are developing systems that exploit a lack of legislation to the disadvantage of data creators. Due to the tremendous impact of those systems in terms of privacy violation, shouldn't there be stricter laws for ML System development or do you have a better solution?
1. Do you have an idea or some kind of concept for a security policy that would be sufficiently abstact to be understandable for humans as well as  sufficiently low  abstract to be understandable for machines?


## Summary
_approximately 250 words_


## Mind Map

* Add your png file here please.
* Please name your png file as `<alsc>_mind-map.png`.

## Question
Do you think that penetration testing should be a mandatory step in ML development to ensure secure systems and if so, should this be ensured by law?

## Takeways
I really liked the idea of combining concepts of different fields with data science and especially machine learning. Due to the rapid development of data science and ML it's highly nescesarry to keep the source of data and especially the humans behind it in mind. By applying concepts from other areas developers can use those insights to develop safer tools in terms of privacy. 
