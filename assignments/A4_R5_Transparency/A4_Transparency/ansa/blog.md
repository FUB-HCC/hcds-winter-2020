# Title of your post
> **Name:** `ansa` Anil S.
> **Session:** [05 Exercise - Transparency](https://github.com/FUB-HCC/hcds-winter-2020/wiki/05_exercise)   
----

## R5 - Reflection
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> **Book:** "Interpretable machine learning. A Guide for Making Black Box Models Explainable" by Molnar

### 🗨️&nbsp; "How does the reading inform your understanding of human-centered data science?"  
_Answer in at least 2-3 complete sentences_

The article gives an overview of the taxonomy of interpretability methods.

The criteria are:
1. intrinsic or post hoc?
1. result of the interpretation method
1. Model-specific or model-agnostic?
1. local or global?

Each criteria lists classes of interpretation. This can be viewed as a little checklist to see how a model classifys in terms of interpretability.
This is crucial for a human-centred design perspective because to use a model wise we have to understand it. Which also means we have to interpret it.



### ❓&nbsp; Questions
1. Can we solve complex problems with only intrinsic machine models?

**Why:** Since we restrict the complexity of the model I asked myself if we can solve complex problems where maybe complex models are better suited.

***

## A4 - Transparency
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> Group: PERSON1 and PERSON2<br>
> Model: NAME OF MODEL<br>

### Summary 

_Please summarize your findings and analyses regarding (1) general understanding, (2) API, (3) ML algorithm and training/test data, and (4) features._

### Openness
...

### Intrinsic interpretability
...

### Algorithmic transparency
...

### Conclusion
_From a human-centered perspective - what do you think about your model and ORES in general?_
