# Assignment 5 - Transparency 
> **Name:** `frra` Franziska R.
> **Session:** [05 Exercise - Transparency](https://github.com/FUB-HCC/hcds-winter-2020/wiki/05_exercise)   
----

## R5 - Reflection
> **Date:** 08.12.2020 - 01:00 PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> **Book:** "Interpretable machine learning. A Guide for Making Black Box Models Explainable" by Molnar

### 🗨️&nbsp; "How does the reading inform your understanding of human-centered data science?"  
This chapter is about how interpretability can be either achieved by restricting a models complexity (also referred to as "intrinsic") or by analyzing the model with certain methods after training (post-hoc). The first thing that came to my mind were Neural Networks. They are commonly used but sometimes way too complex to interpret. And it reminded me of a sentence "complex models are not always te best models". So reducing the complexity or using a simpler model can also achieve good results and in addition it is more interpretable. Due to the fact that not all models (or parameters e.g.) are accessible (black box) there is only the opportunity to analyse the model afterwards, by for example using summary statstics or looking at the used weights. 


### ❓&nbsp; Questions

1. Is there a "best practices" when trying to achieve good results but also model interpretability? 

***

## A4 - Transparency
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> Group: PERSON1 and PERSON2<br>
> Model: NAME OF MODEL<br>

### Summary 

_Please summarize your findings and analyses regarding (1) general understanding, (2) API, (3) ML algorithm and training/test data, and (4) features._

### Openness
...

### Intrinsic interpretability
...

### Algorithmic transparency
...

### Conclusion
_From a human-centered perspective - what do you think about your model and ORES in general?_
