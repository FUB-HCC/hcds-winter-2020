# Title of your post
> **Name:** `jowe` Jonas W.
> **Session:** [05 Exercise - Transparency](https://github.com/FUB-HCC/hcds-winter-2020/wiki/05_exercise)   
----

## R5 - Reflection
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> **Book:** "Interpretable machine learning. A Guide for Making Black Box Models Explainable" by Molnar

### 🗨️&nbsp; "How does the reading inform your understanding of human-centered data science?"  
There are multiple ways to distinguish methods to achieve interpretability such as intrinsic/post hoc which states whether the model was restricted in complexity, which is suitable for simple models such as short decision trees or linear models, or the model was interpreted after the training. Other points are the different kind of results these methods produce, whether the interpretation method is specific to the model or usable on different kind of models (model-agnostic). The last difference brought up is the explenation of a specific prediction from the model or the whole model itself from the interpretation method.


### ❓&nbsp; Questions
1. How commonly used are these methods to achieve interpretability in actual research?

***

## A4 - Transparency
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> Group: PERSON1 and PERSON2<br>
> Model: NAME OF MODEL<br>

### Summary 

_Please summarize your findings and analyses regarding (1) general understanding, (2) API, (3) ML algorithm and training/test data, and (4) features._

### Openness
...

### Intrinsic interpretability
...

### Algorithmic transparency
...

### Conclusion
_From a human-centered perspective - what do you think about your model and ORES in general?_
