# Assignment 5 - Transparency
> **Name:** `goto` Gorgin T.
> **Session:** [05 Exercise - Transparency](https://github.com/FUB-HCC/hcds-winter-2020/wiki/05_exercise)   
----

## R5 - Reflection
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> **Book:** "Interpretable machine learning. A Guide for Making Black Box Models Explainable" by Molnar

### ðŸ—¨ï¸&nbsp; "How does the reading inform your understanding of human-centered data science?"  
Chapter 2.2 aims to explain the _hierarchy of methods regarding "Interpretability"_, their "taxonomy".
_Interpretability_ was earlier explained like this: **It means "the degree to which a human can understand the cause of a decision", that a ML model made.**
So _Interpretability_ is **intrinsic** (_inherent, immanent in the system, a part of how it is built_) or **post-hoc** (_done afterwards, achieved in hind-sight_).
The chapter then jumps to listing five interpretation methods according to their result.
Then, tools for "Interpretability" can be _model specific_ or _model agnositc_. Lastly, if a method explains a single result its _local_, if it explains the whole model's behaviour its _global_.


### â“&nbsp; Questions
1. What came to my mind is that it is quite exhausting when a book has no "golden thread" which it seems not to have here.

**Why:** ...

***

## A4 - Transparency
> **Date:** (Due: 08.12.2020 - 03:00 PM)*<br>
> Group: Xin and Gorgin<br>
> Model: articlequality<br>

### This is part of a group work and is saved [elsewhere](https://github.com/FUB-HCC/hcds-winter-2020/tree/main/assignments/A4_Transparency/xiyu)
