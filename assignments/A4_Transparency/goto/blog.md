# Assignment 5 - Transparency
> **Name:** `goto` Gorgin T.
> **Session:** [05 Exercise - Transparency](https://github.com/FUB-HCC/hcds-winter-2020/wiki/05_exercise)   
----

## R5 - Reflection
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> **Book:** "Interpretable machine learning. A Guide for Making Black Box Models Explainable" by Molnar

### ðŸ—¨ï¸&nbsp; "How does the reading inform your understanding of human-centered data science?"  
Chapter 2.2 aims to explain the _hierarchy of methods regarding "Interpretability"_, their "taxonomy".
_Interpretability_ was earlier explained like this: **It means "the degree to which a human can understand the cause of a decision", that a ML model made.**
So _Interpretability_ is **intrinsic** (_inherent, immanent in the system, a part of how it is built_) or **post-hoc** (_done afterwards, achieved in hind-sight_).
The chapter then jumps to listing five interpretation methods according to their result.
Then, tools for "Interpretability" can be _model specific_ or _model agnositc_. Lastly, if a method explains a single result its _local_, if it explains the whole model's behaviour its _global_.


### â“&nbsp; Questions
1. What came to my mind is that it is quite exhausting when a book has no "golden thread" which it seems not to have here.

**Why:** ...

***

## A4 - Transparency
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> Group: PERSON1 and PERSON2<br>
> Model: NAME OF MODEL<br>

### Summary 

_Please summarize your findings and analyses regarding (1) general understanding, (2) API, (3) ML algorithm and training/test data, and (4) features._

### Openness
...

### Intrinsic interpretability
...

### Algorithmic transparency
...

### Conclusion
_From a human-centered perspective - what do you think about your model and ORES in general?_