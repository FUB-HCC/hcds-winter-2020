# Title of your post
> **Date:** 24.11.2020 - 15:23 PM *(Due: 01.12.2020 - 03:00 PM)*
> **Name:** `seku` Sebastian K.
> **Session:** [04 Exercise - Fairness](https://github.com/FUB-HCC/hcds-winter-2020/wiki/04_exercise)   
----

## R4 - Reflection
> Democast: Mitigating Discrimination and Bias with AI Fairness 360 - Democast #5

### üó®Ô∏è&nbsp; "How does the video inform your understanding of human-centered data science?"  
The video shows a toolkit for analysing data in terms of fairness. Tools like that are a good way to have an objective understanding of the data and possible data bias, so one can make changes and tweak developed systems properly. Nevertheless these tools are defined models that can be faulty, too, so it is nescesarry to maintain, test and change those if nescessary, before working with them on future data.

### ‚ùì&nbsp; Questions
1. What are possible ways to rework existing models towards being unbiased?
2.

***

## A3 - Wikipedia, ORES, and BIAS
Please, put everything regarding `A3` into the `blog.md` file of last week!
