# Title of your post
> **Name:** `mane` Marisa N.
> **Session:** [05 Exercise - Transparency](https://github.com/FUB-HCC/hcds-winter-2020/wiki/05_exercise)   
----

## R5 - Reflection
> **Date:** 06.12.2020 - 10:50 PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> **Book:** "Interpretable machine learning. A Guide for Making Black Box Models Explainable" by Molnar

### ðŸ—¨ï¸&nbsp; "How does the reading inform your understanding of human-centered data science?"  

The reading informs me about the different taxonomies that exist when we are talking about machine model interpretability methods. Therefore, the article introduce four important aspects of how to define specific interpretability methods: first if the method is intrinsic or post hoc, second how does the result of the method look like, third if the method is model-specific or model-agnostic, and last but not least does the method help to better understand a local or global view on the model? For the outcomes of a interpretability methods five different types are introduced: feature summary statistics, feature summary visualization, model internals, data points and intrinsically interpretable models.


### â“&nbsp; Questions
1. Is there already a known interpretation method for each well known, but not intrinsically interpretable machine learning model (e.g. GANs, RNNs, etc.)?
**Why:** The proposed approaches sound great but when it comes to practice and to really complex models, how can we make them interpretable?
1. How can we asses which model we should use when we talk about interpretability?
1. Is there a threshold, when it comes to a trade-off between a more accurate and a intrinsically interpretable model?
1. Why should we use a more complex model in the first place when we later on translate this model to a intrinsically interpretable model for better interpretabllity?
**Why:** These three question arise because I am curious about which model should be choose in the first place and which model is a  "good"/"proper" model for specific cases.

***

## A4 - Transparency
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> Group: PERSON1 and PERSON2<br>
> Model: NAME OF MODEL<br>

### Summary

_Please summarize your findings and analyses regarding (1) general understanding, (2) API, (3) ML algorithm and training/test data, and (4) features._

### Openness
...

### Intrinsic interpretability
...

### Algorithmic transparency
...

### Conclusion
_From a human-centered perspective - what do you think about your model and ORES in general?_
