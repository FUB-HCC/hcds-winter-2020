{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blog.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs07LjFdK1hj"
      },
      "source": [
        "## A5 - Transparency\n",
        "> **Date:** 13.12.2020 - 15:00 PM *(Due: 08.12.2020 - 03:00 PM)*<br>\n",
        "> Group: Franziska and Lukas<br>\n",
        "> Model: draftquality<br>\n",
        "\n",
        "### Summary \n",
        "\n",
        "_Please summarize your findings and analyses regarding (1) general understanding, (2) API, (3) ML algorithm and training/test data, and (4) features._\n",
        "\n",
        "### Openness\n",
        "1. the model (code) is publicly inspectable\n",
        "\n",
        "  * The models code is publicly available on [Github](https://github.com/wikimedia/draftquality).\n",
        "\n",
        "2. the training/test data are publicly inspectable\n",
        "\n",
        "  * Looking at the folder [Datasets](https://github.com/wikimedia/draftquality/tree/master/datasets) in the github repository, we could find the data which is used. But we are not sure if it is used as training/test set, since no information are provided. Looking at all the API calls, they never contained any information of which data was used (except that labeled articles are used). \n",
        "\n",
        "3. individual decisions are reproducible\n",
        "\n",
        "  * The documentation of the repository is rather poor, therefore it is hard to comprehent on how to reproduce the project. It is possible to download and use the model (or use the API for that), but if we would like to edit the code or something else it is hard to look trough all files and all the code. A more detailed documentation would help a lot. \n",
        "\n",
        "4. changes are logged and version controlled\n",
        "\n",
        "  * Since github is used, the changes are all logged. In addition the versions are available, which can be also seen in our notebook from one of the API calls.\n",
        "\n",
        "### Intrinsic interpretability\n",
        "* Intrinsically interpretable models:\n",
        "  * Models that are interpretable by design\n",
        "  * No post-processing steps are needed to achieve interpretability\n",
        "\n",
        "The Gradient Boosting algorithm with decision trees was used. The decision tree algorithm can be easily interpreted. Gradient Boosting produces a prediction model in the form of an ensemble of weak prediction models (in this case decision trees). It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. These models can get quite complex, and it is questionable if it is intrinsicly interpretable. Since (again) there is no documentation available it is hard to interpret this intrinsically, for that we would have inspected the code in detail. \n",
        "\n",
        "### Algorithmic transparency\n",
        "In terms of algorithmic transparency a lot of information are provided, as it can be seen in one of our API calls (model info). The results provided the information that scikit learn Gradient Boosting is used. Information of the scikit learn Gradient Boosting algorithm can be found on the website of [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). We could also figure out which parameters and values were used in the model. Another output are some statistical metrics which were used to evalute the models performance and results. They used many of the typical metrics for that. \n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Overall my group member and I think the project ORES and the related algorithms to maintain wikipedia are not only very usefull and functional, but also provide a good level of transparency. Besides some mentioned problems with missing documentation the amount of information about the model, the alogorithms it uses, and their performance gives a good insight of the models output and helps to identify potential points of failure. \n",
        "\n",
        "Also we were pleasantly surprised by the level of openness that the ORES provides. By having publically accessible training and test data, as well as model code, the ORES makes it reasonably easy to comprehend it's functionality.\n",
        "\n"
      ]
    }
  ]
}