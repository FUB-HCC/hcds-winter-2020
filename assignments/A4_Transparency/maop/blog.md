# Title of your post
> **Name:** `alsc` Alexa S.
> **Session:** [05 Exercise - Transparency](https://github.com/FUB-HCC/hcds-winter-2020/wiki/05_exercise)   
----

## R5 - Reflection
> **Date:** 06.12.2020 - 19:48 *(Due: 08.12.2020 - 03:00 PM)*<br>
> **Book:** "Interpretable machine learning. A Guide for Making Black Box Models Explainable" by Molnar

### 🗨️&nbsp; "How does the reading inform your understanding of human-centered data science?"  
_Answer in at least 2-3 complete sentences_

The concept of interpretability has a higher significance than I previously assumed. Machine Learning algorithms are not just black boxes, where magic is happening and the result needs to be accepted. It is crucial for us to reconstruct and interpret these decisions. For this, there is a necessity of interpretable models. There are two types of interpretability: ad-hoc or intrinsic.

### ❓&nbsp; Questions
1. I understand the necessity of interpretability, but I wonder if this is not more like looking for a needle in a haystack, since there is no "one-size-fits-all" technique to measure?


**Why:** There is no real consensus about what interpretability is in machine learning. Nor is it clear how to measure it. The only way to appriximize the measurement of interpretability is through different level of evaluation (e.g. application, human and function level evaluation). In addition to that, these levels are only simplifications of other levels, so basically there is no way to include all three aspects. So this reminds me a lot of Googles "auf gut Glück" search, where you try your luck and hope to find appropriate results.

***

## A4 - Transparency
> **Date:** DD.MM.YYYY - HH:MM PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> Group: PERSON1 and PERSON2<br>
> Model: NAME OF MODEL<br>

### Summary 

_Please summarize your findings and analyses regarding (1) general understanding, (2) API, (3) ML algorithm and training/test data, and (4) features._

### Openness
...

### Intrinsic interpretability
...

### Algorithmic transparency
...

### Conclusion
_From a human-centered perspective - what do you think about your model and ORES in general?_
