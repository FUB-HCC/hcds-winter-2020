# Reflection 5
> **Name:** `xiyu` Xin Y.
> **Session:** [05 Exercise - Transparency](https://github.com/FUB-HCC/hcds-winter-2020/wiki/05_exercise)   
----

## R5 - Reflection
> **Date:** 07.12.2020 - 11:47 AM *(Due: 08.12.2020 - 03:00 PM)*<br>
> **Book:** "Interpretable machine learning. A Guide for Making Black Box Models Explainable" by Molnar

### üó®Ô∏è&nbsp; "How does the reading inform your understanding of human-centered data science?"  
_Answer in at least 2-3 complete sentences_

Interpretability can be offered through different methods regarding different respects. And various interpretation methods can be categorized regarding to different criterien. 
Intrisic method for interpretation is normally model specific and is closely related to model features, where as post hoc methods is normally applied after model training and provides more flexibility. On the other hand, the various interpretation methods can also be differentiated according to their results. Some interpretation methods may share common features are related with one other. 

### ‚ùì&nbsp; Questions
1. I am still a little bit confused on difference between explanations and interpretations. In the chapter 2.2 (or generally chapter 2), it seems that the author use them as synonyms. 
<br>**Why:** I have read some general papers on XAI which describes interpretability as intrinsic explanability, therefore, it seems that interpretability is subconcept of explanability. I am unsure how to clearly differentiate them apart from each other or they are actually the synonyms. 

1. Is there any difference between interpretability or interpretation methods for parametric and non-parametric learning models? <br>**Why:** This question is interesting for me because some interpretation methods are based on parametric statistics, and I am wondering if there is something we need to consider of when choose an interpretation method. 

***

## A4 - Transparency
> **Date:** 10.12.2020 - 20:09 PM *(Due: 14.12.2020 - 12:00 PM)*<br>
> Group 5: Xin and Gorgin<br>
> Model: `articlequality`<br>

### Summary 

_Please summarize your findings and analyses regarding (1) general understanding, (2) API, (3) ML algorithm and training/test data, and (4) features._

About _general understanding_: The path of learning started at an article of Wikimedia about [Ores](https://diff.wikimedia.org/2015/11/30/artificial-intelligence-x-ray-specs/) where there was an outdated [link to Ores](https://meta.wikimedia.org/wiki/Objective_Revision_Evaluation_Service), but there was the [up-to-date link](https://www.mediawiki.org/wiki/ORES) available. The style of writing is in a way so even non-technicals could follow, which is nice but also lacks in-depth explanation.
For the _API usage_ we get then to the ["Ores homepage"](https://ores.wikimedia.org/) where the [documentation](https://ores.wikimedia.org/v3/), which is actually a swagger page with only brief comments.

### Openness
...

### Intrinsic interpretability
...

### Algorithmic transparency
...

### Conclusion
_From a human-centered perspective - what do you think about your model and ORES in general?_
