# Assignment 4
> **Name:** `arro` Arne Rolf
> **Session:** [05 Exercise - Transparency](https://github.com/FUB-HCC/hcds-winter-2020/wiki/05_exercise)   
----

## R5 - Reflection
> **Date:** 06.12.2020 - 21:39 PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> **Book:** "Interpretable machine learning. A Guide for Making Black Box Models Explainable" by Molnar

### ðŸ—¨ï¸&nbsp; "How does the reading inform your understanding of human-centered data science?"  
This book chapter about interpretability was a good recap and addition to the content of the lecture.
They mentioned that intrinsic interpretability can be applied to models with short decision trees, so I guess this means the amount of layers of a model. Sadly ,they didn't provide a ballpark figure on how many layers this would be feasible to do.

### â“&nbsp; Questions
1. Is the method "Intrinsically interpretable model" meant to dissect the model layers to be able to look at each individual outcome? If so, is the model to be analyzed recreated as a new second model with the same layer parameters but with the learned weights from the first model?

**Why:** ...

***

## A4 - Transparency
> **Date:** 06.12.2020 - 21:39 PM *(Due: 08.12.2020 - 03:00 PM)*<br>
> Group: Arne Rolf (`arro`) and Malina S. (`masc`)<br>
> Model: `goodfaith`<br>

### Summary 

_Please summarize your findings and analyses regarding (1) general understanding, (2) API, (3) ML algorithm and training/test data, and (4) features._

### Openness
...

### Intrinsic interpretability
...

### Algorithmic transparency
...

### Conclusion
_From a human-centered perspective - what do you think about your model and ORES in general?_
