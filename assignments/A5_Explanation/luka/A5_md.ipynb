{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A5.md",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Ye2yXVVkn2"
      },
      "source": [
        "# Assignment 5\n",
        "> **Date:** 16.11.2020 - 18:35 PM *(Due: 11.01.2020 - 03:00 PM)*\n",
        "> **Name:** `frra` Franziska R., `luka` Lukas K.\n",
        "> **Session:** [07 Exercise - Explanations](https://github.com/FUB-HCC/hcds-winter-2020/wiki/07_exercise)   \n",
        "----\n",
        "\n",
        "## A5 - Explanations\n",
        "\n",
        "### Task 1: Different Explanation Needs\n",
        "\n",
        "#### ORES Scenario\n",
        "\n",
        "The ORES scenario ecosystem contains Wikipedia (Scoring Platform Team), Wikipedians (editors on Wikipedia), Wikipedia users, people that analyse articles with ORES and of course the ORES system itself.\n",
        "\n",
        "For this example the 6 different agent roles are distributed as follows, with some overlap between the categories.\n",
        "\n",
        "First the **Creator** role applies to Wikipedia, or more specifically the \"Scoring Platform Team\" and its employees/members, who developed the system as *implementers* and also own it's intellectual property (*owners*). Next the **Operators** are the Wikipedians or other people/machines that analyse wikipedia articles, who manage the in- and output of the model. Since the editing and potential deletion decisions are also made by the former, Wikipidians also take the role of the **Executer**, who have the responsibility of making a decision based on the output of the model. The **Decision Subjects** are all the Wikipedia users, since they may or may not receive certain information. The **Data Subject** is represented again by the Wikipedians and their articles, on account of the model being trained by them. Finally the role of the **Examiner** is also taken by the Wikipedians, since they are the ones that discover issues with the model (e.g. bias).\n",
        "\n",
        "#### Reflection\n",
        "\n",
        "I think this role-based model would be especially useful in the planning phase of the design process. During this phase it could help with assigning tasks to certain individuals, with the addition of maybe granting certain access to only those, who actually need to have it. Also it might help with deciding what the ML Software actually needs to be able to do, since only really the Creators and Operators (maybe also Examiners) have to work with the software.\n",
        "\n",
        "### Task 2: Explanation method: LIME\n",
        "\n",
        "_LINK to your annotated notebook here_\n",
        "\n",
        "1. **What documents did you chose?**\n",
        "\n",
        "The documents we chose were randomly selected from the test dataset.\n",
        "\n",
        "The first document we chose is number 650. This document was classified as christian with a probability of 71%. Looking at the lime explanation with the highlighted words it can be clearly regocnized that this text is christian, since the word \"Christians\" is often mentioned. \n",
        "\n",
        "As the second document we chose document number 702. This was also classified as christian with a probability of 74%. Looking at the highlighted words which were used for classification, the word \"church\" played an important role. But there are also a lot of words highlighted like \"the\" or \"to\" which are clearly not only used by christians. But these words have a lower weight. \n",
        "\n",
        "The last document we chose is number 459 which is predicted as class atheism with 74% probability. This result is hard to comprehend, since the keywords could also be used from a christian. But the prediction is true anyways. \n",
        "\n",
        "\n",
        "2. **What did you learn about the model?**\n",
        "\n",
        "Using lime made it easier to understand with which weight a feature was used, which made the interpretation of the results a lot easier. Due to the fact that it was possible to see which features were used, we could remove some words which belonged to the header for example. Feature selection plays an important role for the training of a model and to know which features have a bigger (or lesser) influence on the prediction makes it possible to adapt the model. \n",
        "\n",
        "\n",
        "3. **How well do you think the classifier works? Why?**\n",
        "\n",
        "As mentioned earlier the model uses also the email headers, footers and quotes, which made the prediction not as precise as it could be. We decided to remove those attributes for the 3 documents we chose to see if the results improve. \n",
        "\n",
        "In terms of the first 2 documents it made sense that they were classified as christian, but in addition we saw that the last document contained highlighted words which are not exclusively used by atheists. Still it was correctly classified. So the model did a good job.\n",
        "\n",
        "Through the usage of lime explanations it was possible for us to quickly recognize that for example the headers are used, which could be then removed afterwards to improve the prediction or at least change the predictions a bit. \n",
        "\n",
        "4. **For what role(s) (from task 1) are LIME explanations useful? Why?**\n",
        "\n",
        "The roles which could profit the most from LIME explanations are in our opinion the Operators and the Examiner. They get in touch with the model itself. The operators manage in- and output from the model, so knowing which features are used can improve the model accuracy. Also the Examiner can discover issues with the model by using the LIME explanations.\n",
        "\n",
        "5. **How useful is LIME for a non-data-scientist (e.g. non-ml-experts or designer)? Why?**\n",
        "\n",
        "LIME is a really useful tool to get explanations from a model, without knowing how it actually works (the algorithm itself). Nevertheless a Data Scientist is required to build the model and also program the LIME pipeline. But only for the task of analyzing the results, a non-Data Scientist can also interpret them easily, due to the visualizations. \n",
        "\n",
        "\n",
        "6. **What question types is LIME able to answer? Why?** \n",
        "\n",
        "LIME makes it possible to create explanations for several classifier predictions. It gives an overview of the used weights and features, which makes the preprocessing and/or post-processing steps of the data easier. Also the results are more easily to interpret and require not much coding."
      ]
    }
  ]
}