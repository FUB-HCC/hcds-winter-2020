# Title of your post
> **Date:** 24.11.2020 - 15:23 PM *(Due: 01.12.2020 - 03:00 PM)*
> **Name:** `alsc` Alexa S.
> **Session:** [04 Exercise - Fairness](https://github.com/FUB-HCC/hcds-winter-2020/wiki/04_exercise)   
----

## R4 - Reflection
> Democast: Mitigating Discrimination and Bias with AI Fairness 360 - Democast #5

### üó®Ô∏è&nbsp; "How does the video inform your understanding of human-centered data science?"  

The video explains how algorithms can be evaluated in terms of their fairness in practice. For this purpose, metrics are offered that state whether the algorithm discriminates against individuals or entire groups or not. Strategies are also presented on how to prevent discrimination or how to counteract discrimination retrospectively. That is why the "AI Fairness 360" framework is presented, which makes it easy to integrate fairness directly into the data analysis pipeline.

### ‚ùì&nbsp; Questions
1. Where has the framework been used in practice and how can success of its use be assessed?
1. How can we identify the protected feature if it is not obvious?

***

## A3 - Wikipedia, ORES, and BIAS
Please, put everything regarding `A3` into the `blog.md` file of last week!