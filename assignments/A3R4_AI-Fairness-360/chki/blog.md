# Mitigating Discrimination and Bias with AI Fairness
> **Date:** 25.11.2020 - 9:00 PM *(Due: 01.12.2020 - 03:00 PM)*
> **Name:** `chki` Christopher K.
> **Session:** [04 Exercise - Fairness](https://github.com/FUB-HCC/hcds-winter-2020/wiki/04_exercise)   
----

## R4 - Reflection
> Democast: Mitigating Discrimination and Bias with AI Fairness 360 - Democast #5

### üó®Ô∏è&nbsp; "How does the video inform your understanding of human-centered data science?"  
I learned that there are different metrics to measure bias and different algorithms to mitigate bias. These algorithms can be grouped into different groups (pre-, in-, post-processing) depending on where they are applied in the machine learning pipeline. These algorithms and their performance have to be tested and compared for the data set in question as some might perform better than others. It is also important to train the model from time to time with recent data and check for biases again. The reason is, that a model that perfoms good regarding bias on old data can perform worse on more recent data (drift).

### ‚ùì&nbsp; Questions
1. Can reducing the bias for an unprivileged group increase/introduce bias for other groups? (Probably not, because we try to equalize the groups of a protected attribute.)
1. Can reducing the bias for a protected attribute increase/introduce bias for other protected attributes?
1. How do I know which attributes to choose as protected attributes when working with new data? I assume that gender, age, migrant background, etc. are quite obvious to check but there might be some other attributes that are less obvious.

***

## A3 - Wikipedia, ORES, and BIAS
Please, put everything regarding `A3` into the `blog.md` file of last week!