# Fairness
> **Date:** 01.12.2020 - 12:23 PM *(Due: 01.12.2020 - 03:00 PM)*
> **Name:** `frra` Franziska R.
> **Session:** [04 Exercise - Fairness](https://github.com/FUB-HCC/hcds-winter-2020/wiki/04_exercise)   
----

## R4 - Reflection
> Democast: Mitigating Discrimination and Bias with AI Fairness 360 - Democast #5

### üó®Ô∏è&nbsp; "How does the video inform your understanding of human-centered data science?"  
This demo was really interesting. He introduced several bias mitigation algorithms and metrics which are provided by AI Fairness 360. In his demo he used open source datasets like the adult income dataset e.g. I already used this dataset a lot of times, but never thought about bias in this dataset. So I thought "ok this is a commonly used dataset but no one ever talks about how for example the gender is a biased feature". It was nice to see that it is possible to reduce the bias to minimum to get a fair prediction. In terms of "human-centered DS" I think this is a great open source tool to support fairness in a prediction and reduce bias in a dataset. It is possbile to select protected classes in which e.g. a privileged and unprivileged group exists. But this came to my mind: ..

### ‚ùì&nbsp; Questions
1. We need to figure out which class/feature needs to be protected (or contain privileged and unprivileged values). How can we do this? In terms of gender or geographical background, as an example, one would suspect a certain bias, but if it is not that clear, it could be rather hard to determine this. 
1. Since he uses several metrics, does it make sense to use all of them at first and select the best one afterwards? 

***

## A3 - Wikipedia, ORES, and BIAS
The blof file can be found here: [Assignment 3](https://github.com/FUB-HCC/hcds-winter-2020/blob/main/assignments/A3_Bias/frra/blog.md)
