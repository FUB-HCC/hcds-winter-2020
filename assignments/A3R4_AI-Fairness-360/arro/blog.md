# Assignment 3 - Reading 4
> **Date:** 01.12.2020 - 13:53 PM *(Due: 01.12.2020 - 03:00 PM)*
> **Name:** `arro` Arne Rolf
> **Session:** [04 Exercise - Fairness](https://github.com/FUB-HCC/hcds-winter-2020/wiki/04_exercise)   
----

## R4 - Reflection
> Democast: Mitigating Discrimination and Bias with AI Fairness 360 - Democast #5

### üó®Ô∏è&nbsp; "How does the video inform your understanding of human-centered data science?"  
I didn't know there is a post processing solution to mitigate bias in classifier results, when the model is already trained with biased data.
Mr. Natesan mentioned a good advice in the video: It is often tempting to start coding on the project but its best to take some time to think about bias in your data before you get started.

### ‚ùì&nbsp; Questions
1. I wondered why IBM decided to not commercialized parts of the AI fairness platform. I could imagine that there is a market for this form of application. But in the end its a good decision in terms of the open source community and the society.

***

## A3 - Wikipedia, ORES, and BIAS
Please, put everything regarding `A3` into the `blog.md` file of last week!