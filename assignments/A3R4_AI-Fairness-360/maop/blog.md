# Title of your post
> **Date:** 24.11.2020 - 15:23 PM *(Due: 01.12.2020 - 03:00 PM)*
> **Name:** `maop` Marc O.
> **Session:** [04 Exercise - Fairness](https://github.com/FUB-HCC/hcds-winter-2020/wiki/04_exercise)   
----

## R4 - Reflection
> Democast: Mitigating Discrimination and Bias with AI Fairness 360 - Democast #5

### üó®Ô∏è&nbsp; "How does the video inform your understanding of human-centered data science?"  
Fairness plays a fundamental role in Human Centered Data Science, there are plenty of Fairness Metrics to meet the requirements to carry out human-centered data science. Also, I loved how you can basically directly apply the theory (paper: `AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias`) behind the toolkit to implement "fair-acting" algorithms. These algorithms can be divided into three groups: (pre-, in-, and postprocessing) and have different outcomes, depending on when in the pipeline they are applied.

### ‚ùì&nbsp; Questions
1. How do you know, what metrics is the most suitable for a problem? He said that different bias types can be discovered and then eliminated using different fairness metrics and algorithms, but what if you're dealing with multiple, emerging and complex types of bias, instead a single type? How do you know which fairness metric is capturing all aspects?
1. How can one be sure, that eliminating one bias for a group is not leading to the appearance of another bias? I feel like this whole topic is way more complex than I initially thought, and I think there might be a chance that when you're creating fairness for someone, this automatically implies disadvantages for others. This way, you'd be contradicting the actual purpose of creating fairness.

***

## A3 - Wikipedia, ORES, and BIAS
Please, put everything regarding `A3` into the `blog.md` file of last week!

[Assignment 3, Wikipedia, ORES and BIAS](https://github.com/mvrcx/A3-hcds-hcc-bias)
