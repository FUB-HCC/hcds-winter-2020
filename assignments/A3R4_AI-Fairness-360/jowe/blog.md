# Title of your post
> **Date:** 30.11.2020 - 17:00 PM *(Due: 01.12.2020 - 03:00 PM)*
> **Name:** `jowe` Jonas W.
> **Session:** [04 Exercise - Fairness](https://github.com/FUB-HCC/hcds-winter-2020/wiki/04_exercise)   
----

## R4 - Reflection
> Democast: Mitigating Discrimination and Bias with AI Fairness 360 - Democast #5

### üó®Ô∏è&nbsp; "How does the video inform your understanding of human-centered data science?"  

From the lecture I already understood the importance of mitigating bias in data science. This video showed me a pretty interesting toolkit for doing so. There are different algorithms for different steps in the pipeline and for different data sets since some may work better than others in some situations. So overall this democast informed me that there are tools which make the difficult search for bias to minimize its impact a bit easier.

### ‚ùì&nbsp; Questions
1. In cases that aren't as obvious as gender, race, etc. how do you decide what features need to be protected? And how can you guarantee that focusing on reducing the bias for some group will not create bias against another one due to the actions taken?

***

## A3 - Wikipedia, ORES, and BIAS
Please, put everything regarding `A3` into the `blog.md` file of last week!