# Title of your post
> **Date:** 24.11.2020 - 15:23 PM *(Due: 01.12.2020 - 03:00 PM)*
> **Name:** `alsc` Alexa S.
> **Session:** [04 Exercise - Fairness](https://github.com/FUB-HCC/hcds-winter-2020/wiki/04_exercise)   
----

## R4 - Reflection
> Democast: Mitigating Discrimination and Bias with AI Fairness 360 - Democast #5

### üó®Ô∏è&nbsp; "How does the video inform your understanding of human-centered data science?"  

The video gave me a general understanding of how the IBM AI Fairness 360 works. They presented multible algorithms that each have a different take on how to find bias. Also they showed how the priviledged group can vary based on the data set. Therefor women are for example disadvantaged when looking at a credit scoring systems data, but are privileged when it comes to the likelihood of repeated criminal offens. 

### ‚ùì&nbsp; Questions
1. Is it always the case that you can assume that a group is unpriviledged just because the data seems to indicate it, since for example men are unpriviledged regarding multible criminal offenses, but are actually statistically more aggressive than women ? 

2. If not, how would it be possible to still determine whether there is bias and a priviledged group?

***

## A3 - Wikipedia, ORES, and BIAS
Please, put everything regarding `A3` into the `blog.md` file of last week!
