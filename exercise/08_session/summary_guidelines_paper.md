# Summary Questions - Paper: Guidelines for Human-AI Interaction

* By reading the guidelines I noticed that the AI-infuced system should give feedback in many places. For example in G11 and G16. In G16 it is mentioned that the system should give this feedback Immediately. I wonder if this immediate feedback may could interrupt the user experience?
* Why did the team from Microsoft choose not to develop a checklist ? Is it even convenient to create a checklist for interactive AI systems?
   * Motivation: There is a good book (The Checklist - Atul Gawande) where the author explains that checklists are a good tool to keep human errors in check. Especially in fields where errors made by humans are fatal like the construction of airplanes. It may not be that interactive AI systems are making mistakes on that level but actually every system can benefit from reducing errors.
* Some of the guidelines should be easy to implement such as G1, G2 and G18 but others are way more complicated such as G5 and G6. There I ask myself if the description ("G6: Match relevant social norms. Ensure the experience is delivered in a way that users would expect, given their social and cultural context.") falls a bit short and leaves room for interpretation on what someone expects.
* Guideline G5 says "Match relevant social norms". I stumbled across that guideline for a couple of reasons. It seemed pretty general and not strongly related to AI. A couple of questions arised: What social norms are important and how should they be taken into account (in my specific context)? Is there related work or a list to refer to? Does this lead into the direction of personalized user experiences (or at least different interfaces tailored to needs of certain user groups)?
* Although most of the rules are very understandable, some leave room for interpretation. Taking the rules "Match relevant social norms" or "Mitigate social biases" is something that is not easily to estimate.
* How can we ensure the correct social and cultural context? The same goes with social biases. This is more of a question of ethics and probably cannot be answered that easily.
* Not really about my own sentences: At first, I got G13 (Learn from user behavior) wrong. It says, that a core element of the software is supposed to use AI to incorporate user's decisions, like a route planner learns from what routes the user selects. I thought it suggests to "extend the AI-system with more AI": The navigation system could also learn when I tend to miss directions (lame example: After work I am tired) and hence tell me instructions earlier, speak louder and slower etc., or tell my radio to play slower songs on Monday morning because I tend to speed then, etc.
* Could it be useful to reach into domains usually not part of the system?
* Maybe that's silly but I found it interesting :)
* How much controll should a user get over adjusting the system, since it might also be overwhelming and take away from the user experience, when asked too often to validate a system produced prediction? (The AI system could feel very dependant on the users verfication, which might make the whole system feel obsolet)
* Should these guidelines not also apply for non-AI systems (i.e. for all decison-making or decison-supporting systems)?
     * I think that with AI systems there is a greater need for more specific design guidline for such systems. These systems become more and more complex and thus need better interfaces. But also non-AI systems can be complex and not transparent for the end-users. So I think these guidelines should be applicable for other applications.
 * How can "Mitigate social biases" be achieved? How will cultural fairness be insured?
 * As the guide is a good tool to develop new interfaces, is there a possibility to automate verification of specific rules to for example reduce the effect of bias which is given by manual tests?
 *  I am curious about how a guideline for traditional HCI UI looks like, because the guideline seems to be too general. I can kinda understand that the research group may want to make it applicable for different AI-based system, but I don't know what considerations made specifically based on AI in formulation of this guideline. I would like to see how a Human-AI system guideline differs from a traditional one
 * Combining G4 and G5, I currently don't know how a human-AI interaction system can show contextually relevant information and match social norms. For example social norms are heavily based on geograpical as well as social information, and sometimes they can be completely different (or even opposite). Does it mean that we should design completely different Human AI system for people from different social/cultural backgrounds? If not, how an AI system match the social norms.
 * For the part "when wrong": support "sufficient" ... seems a liitle bit vague for me. Is there any measurements or framework to see if a support is sufficient enough?
 * The category of "when wrong" (G7-G11) seems particularly important to me. Therefore, I wonder if there is a prioritization in these guidelines, as I can imagine that not every one of these guidelines gets attention considering very short development times. A prioritization would help establish a ranking of importance in order to cover the most important aspects.