# Summary Questions Redaing Book

1. In the case of the AMS Algorithm the question arises in me if it is not more of a conceptual error in matter of how and for which 

purpose the system is used for then the output it generates? In our current society it is not a secret that some groups of people

are disadvantaged based on some attributes they have (or they get assigned). Isn't it good that the algorithm mirrors the reality? 

Could this information not be used to have a good impact on the society. For example the AMS reasons investigate the circumstances why 

these group of people has a lesser chance to get integrated to the job market and create initiatives to help them. 

***

1. Was there someone in the AMS project who had doubts from a ethical standpoint about the data that they used?

***

1. Systems that are developed for decision making or for helping in the decision making process can lead to biased and unfair decisions. A system could be unfair but actually represents the real world correctly (for example becauese there is a systematic discrimination). Depending on a use case, can an unfair system be actually more useful? 
1. Is it the systems task to fight systematic discrimination or should these systems rather be a sign to take measures in the real world.
1. Since such a system can have a huge inpact on society, developers have a high social responsibility. Building fair systems (with a high impact) could actually be a way to fight unfairness in the real world. My question is, who decides what is fair and what is unfair? There are most likely many definitions and also a prevailing opinion in society. The latter could also be influenced (for examply by media) and follow ideological goals which might conflict with (objective?) definitions. 

***

1. How can we prevent such a bias? When is system transparent enough?
2. Ethic comes from humans and not from machines. How can you check if data is biased? Can a data set ever be evaluated without prejudice so that there is no discrimination?

***

1. Again, I wonder if it is even possible to create an algorithm free of bias and such. The argument that the AMS-algorithm simply incorporates the existing injustice of the job-market sounds valid: If all employers in Austria favour men, then the algorithm would give wrong output if it suggests hiring women?
1. Continuing the topic from the first question: Would it make sense to create some sort of preceding algorithm that shows something like  the probability of data being biased or creating unfair output?

***

1. While the authors raise very valid points about specific problems of the AMS Algorithm I am unsure whether I agree with the points they raise about whether a "workfare" state is morally good or bad or whether putting job seekers in a category of low chance of employment may be a self fulfilling prophecy. While these points are surely worth discussing, the question it raises for me is, if every person in every position should be responsible or moraly obligated to break biased structures in their society. In my opinion, whether the "workfare" state is the "right" state structure should not be a question influencing the creators of descriptive algorithms like the one at the AMS in their work. The goal of the algorithm was to predict the chance of a specific applicant to get hired in a given timeframe in the current society. If the system, which the algorithm describes is biased against groups of people, shouldn't the algorithm display that? It is objectively wrong to not hire applicants based on factors like age, gender, etc. but in my opinion the authors mix up their valid criticism of the inner workings of the algorithms with a system critique which may not have a place in this discussion but should and must be continued in another seperate place. Whether this is the right approach and leads to actionable results rather than stopping the use and improvements of such algorithms due to systematic problems which aren't influenceable in the same timeframe and by the same people, is a question worth thinking about and discussing, in my opinion.

***

* How much more bais is an reasonably well implemented algorithm (meaning it doesn't overly discriminate against large groups e.g. woman) compared to humans doing the same job and should a certain level of bias be acceptable in an algorithm, since it might at least be less bias than it's human counter part would be ?

* How much easier might it be to attack bias within alorithms, compared to attacking bias within the mindset of the many workers, that would otherwise do the job ?

***

1. Just as the system is biased, so are people. So how can we build systems that do not prepare and propose decisions that are biased, but rather point out to people their own bias? 

1. Who decides whether and how such systems may be used? The use of such systems can, as we see, have far-reaching consequences for people who themselves have neither control nor insight into these systems.

***

1. Is there a way to scientifically prove the impartiality of such an algorithm? Can we demolish the boundary between empirical observations and questionable decisions and categorizations of the system?
1. More transparency inevitably means a longer development time. When is a system transparent enough to be free of prejudices and bias? 
1. How to take the pressure off the development team in order to enable the selection of suitable data as the basis of bigger data science projects?

***

1. Who developed the algorithm? Did the consist in comupter scientists only or was there people with other backgrounds involved?
1. I wondered why the algorithm did not include the job seekers needs but only their demographics? Wouldn't the classification  be more useful if it integrated the job seeker's personal struggles such as discrimination (if they belong to a minority group), health issues (in case of chronic illnesses)... ?

***

1. Would quality assurance methods such as reviews help prevent bias in data by transferring individual responsibility to an independent group of people? 
2. Is it perhaps necessary for teams of developers of a system to belong to minorities or discriminated groups themselves in order to develop an unbiased system, or could this task also be achieved by, for example, a committee of minorities engaged in reviews.

***

1. As mentioned in the article, the author criticized that the data collection and data cleaning processes are not well documented so that the data analytic process is not transperent and the algorithm may learn bias from data history. Combined with what we learned from the lecture, bias could be imitigated by conciouslly data preparation and increasing explanation as well as interpretability. But since human weselves all have somehow bias, to what extend can the algorithm/decision system REALLY "corrected" by human? 
1. Is there any evaluation metric for measuring bias? Should the algorithm be 0-tolerant towards bias or there is somehow acceptance rate? 
1. I assume we need to sacrifice some efficiency (or maybe even accuracy) for unbiased algorithm design, is there any tradeoff between the so-called "conflicting" goals?
1. How we differ bias and preference in a system? For example as a non-German native speaker I will be definitely disdvantaged when competing with a native speaker for a specific job such as journalist. I personally would not take it as bias/discrimination, it is just a specific preference for specific background/competencies. But still the algorithm outcome maybe considered as unfair if we use the statistical measure. In this case, what will we see as preference; what will we see as bias? Is there any discussion on this issue in academic/practices?
